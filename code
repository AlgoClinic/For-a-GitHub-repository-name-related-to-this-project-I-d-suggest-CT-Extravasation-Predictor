"""
Complete Code for Contrast-Induced Extravasation Prediction Model

This code implements a machine learning model for predicting the risk of contrast media extravasation during intravenous injection.
The workflow includes: data preprocessing, descriptive statistics, feature selection, model training and evaluation, performance validation, and model interpretation.

For submission to Radiology journal
"""

# ===============================
# Part 1: Data Preprocessing
# ===============================

import pandas as pd
import numpy as np
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from scipy.stats import shapiro, chi2_contingency, mannwhitneyu
from statsmodels.stats.outliers_influence import variance_inflation_factor
import logging
import matplotlib.pyplot as plt
import seaborn as sns
import os

def preprocess_data(file_path=None):
    """
    Preprocess raw data, including variable encoding, data splitting, and feature transformation
    
    Parameters:
    file_path: Data file path, default path used if None
    
    Returns:
    X_train, X_test, y_train, y_test, feature_names: Processed training and test sets
    """
    # Set up logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logging.info("Data preprocessing started.")

    # Read data
    file_path = file_path or "/Users/username/Documents/DATA.csv"
    df = pd.read_csv(file_path)
    if df.isnull().any().any():
        logging.warning("Data contains missing values.")
    else:
        logging.info("No missing values detected in the dataset.")
    logging.info(f"Data loaded from {file_path}.")

    # Target variable
    y = df['group']

    # 1. Binary variable recoding
    df['gender'] = df['gender'].map({1: 0, 2: 1})  # 1=Male, 2=Female → 0=Male, 1=Female
    df['needle_placement'] = df['needle_placement'].map({1: 0, 2: 1})  # 1=Left, 2=Right → 0=Left, 1=Right
    logging.info("Binary variable recoding applied: gender (1=Male->0, 2=Female->1), "
                "needle_placement (1=Left->0, 2=Right->1). Other binary features retained original 0/1 encoding.")

    # 2. Data splitting (80:20, 400 training examples, 100 testing examples)
    X_raw = df.drop(columns=['group'])
    X_train_raw, X_test_raw, y_train, y_test = train_test_split(X_raw, y, test_size=0.2, stratify=y, random_state=42)
    logging.info(f"Data split: Train={X_train_raw.shape}, Test={X_test_raw.shape}")
    print(f"Train extravasation proportion: {(y_train == 1).mean():.2%}")
    print(f"Test extravasation proportion: {(y_test == 1).mean():.2%}")

    # 3. Variable classification (based on training set)
    binary_features = ['chemotherapy_history', 'needle_type', 'needle_insertion_depth', 
                      'other_puncture', 'gender', 'malignancy', 'diabetes', 'hypertension', 
                      'needle_dwell_time', 'needle_prior_use', 'pre_procedure_pain', 'needle_placement', 'patient_origin']
    categorical_features = ['vein_quality', 'needle_site', 'needle_gauge', 'active_ingredient']
    continuous_features = ['temp_diff', 'age', 'bmi', 'contrast_dose', 'injection_rate']

    # 4. Merge sparse categories (based on training set only)
    contingency_table = pd.crosstab(X_train_raw['vein_quality'], y_train)
    chi2, p, _, _ = chi2_contingency(contingency_table)
    logging.info(f"Chi2 test for vein_quality vs group (train): chi2={chi2:.2f}, p={p:.3f}")
    if X_train_raw['vein_quality'].value_counts().get(3, 0) < 0.1 * len(X_train_raw) and p > 0.05:
        X_train_raw['vein_quality'] = X_train_raw['vein_quality'].replace(3, 2)
        X_test_raw['vein_quality'] = X_test_raw['vein_quality'].replace(3, 2)
        logging.info("vein_quality: Category 3 merged into 2 due to low sample size and non-significant association in training set.")

    # 5. Descriptive statistics (separate for training and test sets) and distribution comparison
    print("Training set - Binary features distribution:\n", X_train_raw[binary_features].apply(lambda x: x.value_counts(normalize=True)).T)
    print("Training set - Categorical features distribution:\n", X_train_raw[categorical_features].apply(lambda x: x.value_counts(normalize=True)).T)
    for col in continuous_features:
        stat, p = shapiro(X_train_raw[col])
        if p > 0.05:
            print(f"Train {col}: Mean={X_train_raw[col].mean():.2f}, SD={X_train_raw[col].std():.2f} (Normal, p={p:.3f})")
        else:
            print(f"Train {col}: Median={X_train_raw[col].median():.2f}, IQR={X_train_raw[col].quantile(0.75) - X_train_raw[col].quantile(0.25):.2f} (Non-normal, p={p:.3f})")
    print("Test set - Binary features distribution:\n", X_test_raw[binary_features].apply(lambda x: x.value_counts(normalize=True)).T)
    for col in continuous_features:
        stat, p = mannwhitneyu(X_train_raw[col], X_test_raw[col])
        logging.info(f"{col} train vs test distribution: Mann-Whitney U p={p:.3f}")

    # 5.1 Group distribution preview
    print("\nPreview of Training Set Distribution by Group:")
    for col in binary_features + categorical_features:
        crosstab = pd.crosstab(X_train_raw[col], y_train, normalize='columns')
        print(f"{col} by group:\n{crosstab}\n")
    for col in continuous_features:
        g0_median = X_train_raw[col][y_train == 0].median()
        g1_median = X_train_raw[col][y_train == 1].median()
        print(f"{col}: Group 0 Median={g0_median:.2f}, Group 1 Median={g1_median:.2f}")

    # 5.2 Univariate analysis
    print("\nUnivariate p-values (Training Set):")
    for col in X_train_raw.columns:
        if col in binary_features + categorical_features:
            _, p = chi2_contingency(pd.crosstab(X_train_raw[col], y_train))[:2]
            logging.info(f"Univariate test {col} vs group: Chi2 p={p:.3f}")
        else:
            _, p = mannwhitneyu(X_train_raw[col][y_train == 0], X_train_raw[col][y_train == 1])
            logging.info(f"Univariate test {col} vs group: Mann-Whitney U p={p:.3f}")

    # 6. OneHot encoding (based on training set)
    encoder = OneHotEncoder(drop='first', sparse_output=False)
    encoded_train = encoder.fit_transform(X_train_raw[categorical_features])
    encoded_test = encoder.transform(X_test_raw[categorical_features])
    encoded_columns = encoder.get_feature_names_out(categorical_features)
    logging.info(f"OneHot encoding completed for {categorical_features}.")

    # 7. Standardization (based on training set)
    scaler = StandardScaler()
    cont_train = scaler.fit_transform(X_train_raw[continuous_features])
    cont_test = scaler.transform(X_test_raw[continuous_features])
    logging.info("Continuous features standardized.")

    # 8. Combine feature matrices
    X_train = np.hstack((X_train_raw[binary_features].values, encoded_train, cont_train))
    X_test = np.hstack((X_test_raw[binary_features].values, encoded_test, cont_test))
    feature_names = binary_features + list(encoded_columns) + continuous_features

    # 9. Outlier detection
    def detect_outliers(df, col):
        Q1, Q3 = df[col].quantile([0.25, 0.75])
        IQR = Q3 - Q1
        lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR
        return df[(df[col] < lower) | (df[col] > upper)][col]

    for col in continuous_features:
        outliers_train = detect_outliers(X_train_raw, col)
        if not outliers_train.empty:
            logging.warning(f"Outliers in train {col}: {len(outliers_train)} values "
                          f"(Range: {outliers_train.min():.2f} to {outliers_train.max():.2f}); "
                          f"retained as they represent <5% of data.")

    # 10. Multicollinearity check
    vif_data = pd.DataFrame(X_train, columns=feature_names)
    vif = pd.DataFrame()
    vif["Feature"] = feature_names
    vif["VIF"] = [variance_inflation_factor(vif_data.values, i) for i in range(vif_data.shape[1])]
    logging.info(f"VIF calculation (train):\n{vif}")
    high_vif = vif[vif["VIF"] > 5]
    if not high_vif.empty:
        logging.info(f"Features with VIF > 5 detected in training set:\n{high_vif}. "
                   "These were retained to prioritize predictive performance over causal inference.")

    # 11. Output results
    print("Data preprocessing complete.")
    print("Feature names:", feature_names)
    print("Train feature shape:", X_train.shape)
    print("Test feature shape:", X_test.shape)
    logging.info(f"Data preprocessing completed. Train shape: {X_train.shape}, Test shape: {X_test.shape}")

    # 12. Visualization
    plt.figure(figsize=(12, 6))
    for i, col in enumerate(continuous_features, 1):
        plt.subplot(2, 3, i)
        sns.boxplot(x=y_train, y=X_train_raw[col])
        plt.title(f"{col} by Group")
    plt.tight_layout()
    plt.savefig("boxplot_continuous_by_group.png", dpi=300)
    logging.info("Boxplot of continuous features by group saved as 'boxplot_continuous_by_group.png'.")

    # Save processed data
    train_data = pd.DataFrame(X_train, columns=feature_names)
    train_data['group'] = y_train.values
    test_data = pd.DataFrame(X_test, columns=feature_names)
    test_data['group'] = y_test.values
    train_data.to_csv('processed_train_data.csv', index=False)
    test_data.to_csv('processed_test_data.csv', index=False)
    logging.info("Processed train and test data saved to 'processed_train_data.csv' and 'processed_test_data.csv'.")
    
    return X_train, X_test, y_train, y_test, feature_names


# ===============================
# Part 2: Descriptive Statistics and Table Generation
# ===============================

from scipy.stats import chi2_contingency, mannwhitneyu, fisher_exact

def generate_descriptive_statistics(file_path=None):
    """
    Generate Table 1: Descriptive Statistics and Group Comparison for the full dataset
    
    Parameters:
    file_path: Data file path, default path used if None
    
    Returns:
    table1_full: DataFrame containing descriptive statistics
    """
    # Set up logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logging.info("Generating Table 1: Descriptive Statistics and Group Comparison (Full Dataset)")

    # Read original data
    file_path = file_path or "/Users/username/Documents/DATA.csv"
    df = pd.read_csv(file_path)
    if df.isnull().any().any():
        logging.warning("Data contains missing values.")
    else:
        logging.info("No missing values detected in the dataset.")

    # Recode mappings
    df['needle_type'] = df['needle_type'].map({1: 0, 2: 1})  # 1→0 (Straight), 2→1 (Y-type)
    df['gender'] = df['gender'].map({1: 0, 2: 1})  # 1→0 (Male), 2→1 (Female)
    df['needle_placement'] = df['needle_placement'].map({1: 0, 2: 1})  # 1→0 (Left), 2→1 (Right)

    # Full dataset
    full_data = df
    logging.info(f"Full dataset size: {len(full_data)}, Non-Extravasation: {(full_data['group'] == 0).sum()}, Extravasation: {(full_data['group'] == 1).sum()}")

    # Define variable types and labels
    binary_features = {
        'chemotherapy_history': ['No', 'Yes'],
        'needle_type': ['Straight', 'Y-type'],
        'needle_insertion_depth': ['Not fully inserted', 'Fully inserted'],
        'other_puncture': ['No', 'Yes'],
        'gender': ['Male', 'Female'],
        'malignancy': ['No', 'Yes'],
        'diabetes': ['No', 'Yes'],
        'hypertension': ['No', 'Yes'],
        'needle_dwell_time': ['Not same day', 'Same day'],
        'needle_prior_use': ['No', 'Yes'],
        'pre_procedure_pain': ['No', 'Yes'],
        'needle_placement': ['Left', 'Right'],
        'patient_origin': ['Outpatient', 'Inpatient']
    }
    categorical_features = {
        'vein_quality': ['Good', 'Fair', 'Poor', 'Very poor'],
        'needle_site': ['Elbow', 'Forearm/Upper arm', 'Wrist', 'Hand'],
        'needle_gauge': ['22G', '20G', '18G'],
        'active_ingredient': ['Iodixanol', 'Iomeprol', 'Iopromide', 'Iopamidol']
    }
    continuous_features = ['temp_diff', 'age', 'bmi', 'contrast_dose', 'injection_rate']
    continuous_labels = ['Temperature Difference (°C)', 'Age (years)', 'BMI (kg/m²)', 'Contrast Dose (mL)', 'Injection Rate (mL/s)']

    # Initialize Table 1
    table1_full = pd.DataFrame(columns=['Variable', 'Non-Extravasation (n=248)', 'Extravasation (n=252)', 'P-value'])

    # Binary variables
    for col, labels in binary_features.items():
        crosstab = pd.crosstab(full_data[col], full_data['group'], normalize='columns') * 100
        contingency_table = pd.crosstab(full_data[col], full_data['group'])
        if contingency_table.min().min() < 5:
            oddsratio, p = fisher_exact(contingency_table)
            logging.info(f"{col}: Fisher exact p-value = {p:.3f} (used due to small cell counts)")
        else:
            chi2, p, _, _ = chi2_contingency(contingency_table)
            logging.info(f"{col}: Chi-square p-value = {p:.3f}")
        p_str = f"{p:.3f}" if p >= 0.001 else "<0.001"
        rows = [
            {'Variable': col.replace('_', ' ').title(), 'Non-Extravasation (n=248)': '', 'Extravasation (n=252)': '', 'P-value': p_str},
            {'Variable': f"  {labels[0]} (0)", 'Non-Extravasation (n=248)': f"{crosstab.loc[0, 0]:.1f}% ({int(crosstab.loc[0, 0] * 248 / 100)})" if 0 in crosstab.index else "0.0% (0)", 
             'Extravasation (n=252)': f"{crosstab.loc[0, 1]:.1f}% ({int(crosstab.loc[0, 1] * 252 / 100)})" if 0 in crosstab.index else "0.0% (0)", 'P-value': ''},
            {'Variable': f"  {labels[1]} (1)", 'Non-Extravasation (n=248)': f"{crosstab.loc[1, 0]:.1f}% ({int(crosstab.loc[1, 0] * 248 / 100)})" if 1 in crosstab.index else "0.0% (0)", 
             'Extravasation (n=252)': f"{crosstab.loc[1, 1]:.1f}% ({int(crosstab.loc[1, 1] * 252 / 100)})" if 1 in crosstab.index else "0.0% (0)", 'P-value': ''}
        ]
        table1_full = pd.concat([table1_full, pd.DataFrame(rows)], ignore_index=True)

    # Categorical variables
    for col, labels in categorical_features.items():
        crosstab = pd.crosstab(full_data[col], full_data['group'], normalize='columns') * 100
        contingency_table = pd.crosstab(full_data[col], full_data['group'])
        chi2, p, _, _ = chi2_contingency(contingency_table)
        if contingency_table.min().min() < 5:
            logging.info(f"{col}: Chi-square p-value = {p:.3f} (small cell counts present; consider category merging for robustness).")
        else:
            logging.info(f"{col}: Chi-square p-value = {p:.3f}")
        p_str = f"{p:.3f}" if p >= 0.001 else "<0.001"
        rows = [{'Variable': col.replace('_', ' ').title(), 'Non-Extravasation (n=248)': '', 'Extravasation (n=252)': '', 'P-value': p_str}]
        for i, label in enumerate(labels):
            idx = i if col == 'vein_quality' else i + 1
            rows.append({
                'Variable': f"  {label} ({idx})",
                'Non-Extravasation (n=248)': f"{crosstab.loc[idx, 0]:.1f}% ({int(crosstab.loc[idx, 0] * 248 / 100)})" if idx in crosstab.index else "0.0% (0)",
                'Extravasation (n=252)': f"{crosstab.loc[idx, 1]:.1f}% ({int(crosstab.loc[idx, 1] * 252 / 100)})" if idx in crosstab.index else "0.0% (0)",
                'P-value': ''
            })
        table1_full = pd.concat([table1_full, pd.DataFrame(rows)], ignore_index=True)

    # Continuous variables
    def detect_outliers(series):
        Q1, Q3 = series.quantile([0.25, 0.75])
        IQR = Q3 - Q1
        lower, upper = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR
        return series[(series < lower) | (series > upper)]

    for col, label in zip(continuous_features, continuous_labels):
        group_0 = full_data[full_data['group'] == 0][col]
        group_1 = full_data[full_data['group'] == 1][col]
        median_0, q1_0, q3_0 = group_0.median(), group_0.quantile(0.25), group_0.quantile(0.75)
        median_1, q1_1, q3_1 = group_1.median(), group_1.quantile(0.25), group_1.quantile(0.75)
        stat, p = mannwhitneyu(group_0, group_1)
        logging.info(f"{col}: Mann-Whitney U p-value = {p:.3f}")
        outliers_0, outliers_1 = detect_outliers(group_0), detect_outliers(group_1)
        if not (outliers_0.empty and outliers_1.empty):
            logging.info(f"{col}: Outliers detected - Group 0: {len(outliers_0)}, Group 1: {len(outliers_1)} (retained for analysis).")
        p_str = f"{p:.3f}" if p >= 0.001 else "<0.001"
        table1_full = pd.concat([table1_full, pd.DataFrame({
            'Variable': [label],
            'Non-Extravasation (n=248)': [f"{median_0:.1f} [{q1_0:.1f}-{q3_0:.1f}]"],
            'Extravasation (n=252)': [f"{median_1:.1f} [{q1_1:.1f}-{q3_1:.1f}]"],
            'P-value': [p_str]
        })], ignore_index=True)

    # Output and save
    print("Table 1: Descriptive Statistics and Group Comparison (Full Dataset, n=500)")
    print(table1_full.to_string(index=False))
    table1_full.to_csv('table1_descriptive_statistics_full.csv', index=False)
    logging.info("Full Dataset Table 1 saved to 'table1_descriptive_statistics_full.csv'.")
    print("\nNote: Values are n (%) for categorical variables and median [Q1-Q3] for continuous variables. P-values <0.05 indicate statistical significance.")
    
    return table1_full


# ===============================
# Part 3: Feature Selection
# ===============================

from scipy.stats import chi2_contingency, mannwhitneyu, fisher_exact
from sklearn.linear_model import LassoCV
from sklearn.metrics import roc_auc_score
import statsmodels.api as sm

def select_features(X_train, y_train, X_test, feature_names):
    """
    Perform feature selection using univariate analysis and LASSO regularization
    
    Parameters:
    X_train, y_train: Training data
    X_test: Test data
    feature_names: List of feature names
    
    Returns:
    X_train_selected, X_test_selected, selected_features: Datasets with selected features and feature names
    """
    # Set up logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logging.info("Feature selection process initiated (using training set only).")

    # Create DataFrame for easier analysis
    train_data_encoded = pd.DataFrame(X_train, columns=feature_names)
    train_data_encoded['group'] = y_train.copy()

    # Define variable types (consistent with Table 1)
    binary_features = ['chemotherapy_history', 'needle_type', 'needle_insertion_depth', 'other_puncture', 'gender', 
                      'malignancy', 'diabetes', 'hypertension', 'needle_dwell_time', 'needle_prior_use', 
                      'pre_procedure_pain', 'needle_placement', 'patient_origin']
    categorical_features = ['vein_quality', 'needle_site', 'needle_gauge', 'active_ingredient']
    continuous_features = ['temp_diff', 'age', 'bmi', 'contrast_dose', 'injection_rate']

    # 1. Univariate statistical analysis (training set)
    p_values_table = {}
    for col in train_data_encoded.columns[:-1]:  # Exclude 'group'
        if col in binary_features:
            crosstab = pd.crosstab(train_data_encoded[col], train_data_encoded['group'])
            if crosstab.min().min() < 5:
                _, p = fisher_exact(crosstab)
                logging.info(f"{col}: Fisher exact p-value = {p:.3f}")
            else:
                _, p, _, _ = chi2_contingency(crosstab)
                logging.info(f"{col}: Chi-square p-value = {p:.3f}")
            p_values_table[col] = p
        elif col in continuous_features:
            group_0 = train_data_encoded[train_data_encoded['group'] == 0][col]
            group_1 = train_data_encoded[train_data_encoded['group'] == 1][col]
            _, p = mannwhitneyu(group_0, group_1)
            p_values_table[col] = p
            logging.info(f"{col}: Mann-Whitney U p-value = {p:.3f}")
        else:  # Handle one-hot encoded variables
            parent_var = col.split('_')[0]
            if parent_var in categorical_features and parent_var not in p_values_table:
                related_cols = [c for c in train_data_encoded.columns if c.startswith(parent_var + '_')]
                if related_cols:
                    # Reconstruct original categorical variable
                    original_var = train_data_encoded[related_cols].idxmax(axis=1).str.replace(parent_var + '_', '')
                    original_var = original_var.astype(int)
                    crosstab = pd.crosstab(original_var, train_data_encoded['group'])
                    if crosstab.min().min() < 5:
                        logging.warning(f"{parent_var}: Small cell counts detected; consider merging categories.")
                    _, p, _, _ = chi2_contingency(crosstab)
                    p_values_table[parent_var] = p
                    logging.info(f"{parent_var}: Chi-square p-value = {p:.3f}")

    # Extract significant variables from training set statistics (p<0.15)
    significant_vars = [col for col, p in p_values_table.items() if p < 0.15]
    logging.info(f"Variables with p<0.15 from training set statistics: {significant_vars}")
    print("Significant variables from training set (p<0.15):", significant_vars)

    # 2. Univariate logistic regression analysis (training set only)
    p_values = {}
    or_values = {}
    conf_intervals = {}
    for col in feature_names:
        X = sm.add_constant(train_data_encoded[[col]])
        model = sm.Logit(train_data_encoded['group'], X).fit(disp=0)
        p = model.pvalues[1]
        or_val = np.exp(model.params[1])
        conf_int = np.exp(model.conf_int().iloc[1])
        p_values[col] = p
        or_values[col] = or_val
        conf_intervals[col] = conf_int
        print(f"{col}: OR = {or_val:.3f} (95% CI: {conf_int[0]:.3f}-{conf_int[1]:.3f}), p-value = {p:.3f}{' *' if p < 0.05 else ''}")
        if col not in continuous_features:
            crosstab = pd.crosstab(train_data_encoded[col], train_data_encoded['group'])
            if crosstab.min().min() < 5:
                _, chi_p = fisher_exact(crosstab)
                logging.info(f"{col}: Fisher exact p-value = {chi_p:.3f}")
            else:
                _, chi_p, _, _ = chi2_contingency(crosstab)
                logging.info(f"{col}: Chi-square p-value = {chi_p:.3f}")

    # Filter significant variables (p<0.15)
    significant_vars_encoded = [col for col, p in p_values.items() if p < 0.15]

    # Combine significant variables and remove duplicates
    all_significant_vars = list(set(significant_vars + significant_vars_encoded))
    logging.info(f"All variables with p<0.15 after univariate analysis: {all_significant_vars}")
    print("All significant variables (p<0.15):", all_significant_vars)

    # Extract feature indices
    sig_indices = []
    for var in all_significant_vars:
        if var in feature_names:
            sig_indices.append(feature_names.index(var))
        elif var in categorical_features:
            sig_indices.extend([i for i, name in enumerate(feature_names) if var in name])
    sig_indices = list(set(sig_indices))
    X_train_sig = X_train[:, sig_indices]
    X_test_sig = X_test[:, sig_indices]
    sig_feature_names = [feature_names[i] for i in sig_indices]
    logging.info(f"Feature indices extracted for significant variables: {sig_indices}")

    # 3. LASSO selection
    lasso = LassoCV(cv=10, random_state=42, alphas=np.logspace(-8, 3, 200), max_iter=10000)
    lasso.fit(X_train_sig, y_train)
    selected_indices = [i for i in range(len(lasso.coef_)) if lasso.coef_[i] != 0]
    selected_features = [sig_feature_names[i] for i in selected_indices]
    logging.info(f"Features selected by LASSO: {selected_features}")
    logging.info(f"LASSO coefficients for selected features: {lasso.coef_[lasso.coef_ != 0]}")
    logging.info(f"Optimal alpha selected by LASSO: {lasso.alpha_}")
    print("Selected features:", selected_features)
    print("Lasso coefficients:", lasso.coef_[lasso.coef_ != 0])
    print("Optimal alpha:", lasso.alpha_)
    alpha_idx = np.where(lasso.alphas_ == lasso.alpha_)[0][0]
    cv_mse = lasso.mse_path_.mean(axis=1)[alpha_idx]
    cv_mse_std = lasso.mse_path_.std(axis=1)[alpha_idx]
    print(f"Cross-validation MSE at optimal alpha: {cv_mse:.4f} (SD: {cv_mse_std:.4f})")

    # Calculate LASSO model AUC
    y_pred_prob = lasso.predict(X_train_sig)
    auc = roc_auc_score(y_train, y_pred_prob)
    logging.info(f"LASSO model AUC on training set: {auc:.3f}")
    print(f"LASSO model AUC on training set: {auc:.3f}")

    # Update sig_feature_names
    sig_feature_names = selected_features  # Ensure consistency with LASSO selection
    logging.info(f"Updated sig_feature_names after LASSO: {sig_feature_names}")

    # Update training and test sets
    X_train_selected = X_train_sig[:, selected_indices]
    X_test_selected = X_test_sig[:, selected_indices]
    logging.info("Feature selection process completed successfully.")

    # Print X_train_selected and sig_feature_names to confirm matching
    print("X_train_selected shape after LASSO:", X_train_selected.shape)
    print("Length of sig_feature_names after LASSO:", len(sig_feature_names))
    if X_train_selected.shape[1] != len(sig_feature_names):
        raise ValueError(f"Column mismatch: X_train_selected has {X_train_selected.shape[1]} columns, but sig_feature_names has {len(sig_feature_names)} names.")

    # Save selection results
    selected_features_df = pd.DataFrame({
        'Feature': selected_features,
        'Lasso Coefficient': lasso.coef_[lasso.coef_ != 0]
    })
    selected_features_df.to_csv('selected_features_lasso.csv', index=False)
    logging.info("Selected features and coefficients saved to 'selected_features_lasso.csv'.")
    
    return X_train_selected, X_test_selected, selected_features


# ===============================
# Part 4: LASSO Visualization
# ===============================

def visualize_lasso(X_train_selected, y_train, selected_features):
    """
    Generate visualization plots for LASSO feature selection
    
    Parameters:
    X_train_selected: Training data with selected features
    y_train: Training target
    selected_features: List of selected feature names
    
    Returns:
    lasso_cv: Trained LASSO model
    """
    # Set up logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logging.info("Initiating generation of Lasso visualization plots.")

    # Verify consistency
    print("Length of selected_features:", len(selected_features))
    print("X_train_selected shape:", X_train_selected.shape)
    if len(selected_features) != X_train_selected.shape[1]:
        raise ValueError(f"Length mismatch: selected_features ({len(selected_features)}) does not match X_train_selected features ({X_train_selected.shape[1]})")

    # Map one-hot encoded variables to intuitive names
    feature_mapping = {
        'chemotherapy_history': 'Chemotherapy History',
        'needle_type': 'Needle Type',
        'needle_insertion_depth': 'Needle Insertion Depth',
        'other_puncture': 'Other Puncture',
        'gender': 'Gender',
        'malignancy': 'Malignancy',
        'temp_diff': 'Temperature Difference',
        'age': 'Age',
        'needle_dwell_time': 'Needle Dwell Time',
        'needle_prior_use': 'Needle Prior Use',
        'pre_procedure_pain': 'Pre-procedure Pain',
        'needle_placement': 'Needle Placement',
        'patient_origin': 'Patient Origin',
        'diabetes': 'Diabetes',
        'hypertension': 'Hypertension',
        'bmi': 'BMI',
        'contrast_dose': 'Contrast Dose',
        'injection_rate': 'Injection Rate'
    }
    one_hot_mapping = {
        'vein_quality_1': 'Vein Quality (Fair)',
        'vein_quality_2': 'Vein Quality (Poor)',
        'vein_quality_3': 'Vein Quality (Very Poor)',
        'needle_site_2': 'Needle Site (Forearm/Upper Arm)',
        'needle_site_3': 'Needle Site (Wrist)',
        'needle_site_4': 'Needle Site (Hand)',
        'needle_gauge_2': 'Needle Gauge (20G)',
        'needle_gauge_3': 'Needle Gauge (18G)',
        'active_ingredient_2': 'Active Ingredient (Iomeprol)',
        'active_ingredient_3': 'Active Ingredient (Iopromide)',
        'active_ingredient_4': 'Active Ingredient (Iopamidol)'
    }
    mapped_features = [feature_mapping.get(f, one_hot_mapping.get(f, f)) for f in selected_features]

    # LASSO model
    alphas = np.logspace(-8, 3, 200)  # Expanded alpha range with increased steps
    lasso_cv = LassoCV(cv=10, random_state=42, alphas=alphas, max_iter=10000)
    lasso_cv.fit(X_train_selected, y_train)
    logging.info(f"LassoCV completed. Optimal alpha: {lasso_cv.alpha_:.5f}, Minimum MSE: {np.min(np.mean(lasso_cv.mse_path_, axis=1)):.5f}")

    # Calculate LASSO model AUC
    y_pred_prob = lasso_cv.predict(X_train_selected)
    auc = roc_auc_score(y_train, y_pred_prob)
    logging.info(f"LASSO model AUC on training set: {auc:.3f}")
    print(f"LASSO model AUC on training set: {auc:.3f}")

    # Verify lasso_cv.coef_ and selected_features length
    print("Length of lasso_cv.coef_:", len(lasso_cv.coef_))
    if len(selected_features) != len(lasso_cv.coef_):
        raise ValueError(f"Length mismatch: selected_features ({len(selected_features)}) does not match lasso_cv.coef_ ({len(lasso_cv.coef_)})")

    # Determine LASSO retained features
    selected_indices = [i for i in range(len(lasso_cv.coef_)) if lasso_cv.coef_[i] != 0]
    selected_features = [selected_features[i] for i in selected_indices]
    selected_mapped = [mapped_features[i] for i in selected_indices]

    # 1. Coefficient path plot
    plt.figure(figsize=(14, 6))
    coefs = lasso_cv.path(X_train_selected, y_train, alphas=alphas)[1]
    for i in range(coefs.shape[0]):
        if i in selected_indices:
            plt.plot(alphas, coefs[i, :], label=mapped_features[i], linewidth=2.5)
        else:
            plt.plot(alphas, coefs[i, :], color='gray', alpha=0.3, linewidth=1, linestyle='--')
    plt.xscale('log')
    plt.axvline(lasso_cv.alpha_, linestyle='--', color='black', label=f'Optimal λ = {lasso_cv.alpha_:.5f}')
    plt.xlabel('λ (log scale)', fontsize=12)
    plt.ylabel('Coefficient Value', fontsize=12)
    plt.title('Lasso Coefficient Path', fontsize=14)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', ncol=1, fontsize=10, frameon=True)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.savefig('lasso_coefficient_path.png', dpi=300, bbox_inches='tight')
    logging.info("Lasso coefficient path plot saved as 'lasso_coefficient_path.png'.")
    plt.show()

    # List all candidate variables and their coefficients (including non-retained features)
    print("Note: All candidate variables in LASSO with final coefficients:")
    for i, var in enumerate(mapped_features):
        coef = lasso_cv.coef_[i] if i < len(lasso_cv.coef_) else 0
        print(f"{i+1}. {var} {'(Selected)' if var in selected_mapped else '(Not Selected)'} - Coefficient: {coef:.4f}")

    # 2. Cross-validation error plot
    plt.figure(figsize=(14, 6))
    mse_mean = np.mean(lasso_cv.mse_path_, axis=1)
    mse_std = np.std(lasso_cv.mse_path_, axis=1)
    plt.errorbar(np.log10(alphas), mse_mean, yerr=mse_std, fmt='o', color='blue', ecolor='lightblue', 
                capsize=3, markersize=6, label='Mean MSE ± Std')
    min_idx = np.argmin(mse_mean)
    optimal_lambda = alphas[min_idx]
    plt.axvline(np.log10(optimal_lambda), color='red', linestyle='--', label=f'Optimal λ = {optimal_lambda:.5f}')
    plt.text(np.log10(optimal_lambda), max(mse_mean), f'MSE = {mse_mean[min_idx]:.3f}', color='red', ha='right', fontsize=10)
    lambda_1se = alphas[np.where(mse_mean <= mse_mean[min_idx] + mse_std[min_idx])[0][-1]]
    mse_1se = mse_mean[np.where(alphas == lambda_1se)[0][0]]
    plt.axvline(np.log10(lambda_1se), color='red', linestyle=':', label=f'λ (1-SE) = {lambda_1se:.5f}')
    plt.text(np.log10(lambda_1se), max(mse_mean)*0.95, f'MSE = {mse_1se:.3f}', color='red', ha='right', fontsize=10)
    logging.info(f"1-SE λ: {lambda_1se:.5f}, MSE at 1-SE λ: {mse_1se:.3f}")
    print(f"1-SE λ: {lambda_1se:.5f}, MSE at 1-SE λ: {mse_1se:.3f}")
    plt.xlabel('log(λ)', fontsize=12)
    plt.ylabel('Mean Squared Error', fontsize=12)
    plt.title('Lasso Cross-Validation Error', fontsize=14)
    plt.legend(fontsize=10)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.savefig('lasso_cv_error.png', dpi=300, bbox_inches='tight')
    logging.info("Lasso cross-validation error plot saved as 'lasso_cv_error.png'.")
    plt.show()
    
    return lasso_cv


# ===============================
# Part 5: Model Training and Evaluation
# ===============================

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import roc_auc_score, roc_curve, classification_report
from sklearn.model_selection import GridSearchCV
import xgboost as xgb
from scipy.stats import bootstrap
from scipy import stats
import matplotlib.patheffects as path_effects

def train_and_evaluate_models(X_train_selected, X_test_selected, y_train, y_test):
    """
    Train multiple machine learning models and evaluate their performance
    
    Parameters:
    X_train_selected, X_test_selected: Training and test data with selected features
    y_train, y_test: Training and test targets
    
    Returns:
    best_model_name: Name of the best performing model
    models: Dictionary of trained models
    y_pred_tests: Dictionary of test set predictions
    """
    # Set up logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logging.info("Initiating model training and evaluation.")

    # Define models
    models = {
        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
        'Random Forest': RandomForestClassifier(random_state=42),
        'SVM': SVC(probability=True, random_state=42),
        'XGBoost': xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss'),
        'K-Nearest Neighbors': KNeighborsClassifier()
    }

    # Hyperparameter tuning ranges
    param_grids = {
        'Logistic Regression': {'C': [0.01, 0.1, 1, 10, 100, 1000], 'penalty': ['l1', 'l2'], 'solver': ['liblinear']},
        'Random Forest': {'n_estimators': [50, 100, 200, 500], 'max_depth': [None, 5, 10, 20, 30], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 5, 10]},
        'SVM': {'C': [0.01, 0.1, 1, 10, 100], 'kernel': ['rbf', 'linear'], 'gamma': [0.01, 0.05, 0.1, 0.5, 'scale', 'auto']},
        'XGBoost': {'n_estimators': [50, 100, 200, 500], 'max_depth': [3, 5, 7, 9], 'learning_rate': [0.01, 0.05, 0.1, 0.3], 'subsample': [0.6, 0.8, 1.0], 'reg_lambda': [0, 1, 10]},
        'K-Nearest Neighbors': {'n_neighbors': [3, 5, 7, 9, 11, 15, 20], 'weights': ['uniform', 'distance'], 'p': [1, 2]}
    }

    # Assign fixed color to each model
    model_colors = {
        'Logistic Regression': sns.color_palette("Set1", n_colors=5)[0],
        'Random Forest': sns.color_palette("Set1", n_colors=5)[1],
        'SVM': sns.color_palette("Set1", n_colors=5)[2],
        'XGBoost': sns.color_palette("Set1", n_colors=5)[3],
        'K-Nearest Neighbors': sns.color_palette("Set1", n_colors=5)[4]
    }

    # Train and evaluate
    results = {}
    roc_curves_train = {}
    roc_curves_test = {}
    best_params = {}
    y_pred_tests = {}  # Store test set prediction probabilities for DeLong test

    for name, model in models.items():
        logging.info(f"Training model: {name}")
        # Hyperparameter tuning
        grid_search = GridSearchCV(model, param_grids[name], cv=5, scoring='roc_auc', n_jobs=-1)
        try:
            grid_search.fit(X_train_selected, y_train)
            model = grid_search.best_estimator_
            best_params[name] = grid_search.best_params_
            logging.info(f"{name} best parameters: {grid_search.best_params_}")
            print(f"{name} best parameters: {grid_search.best_params_}")
        except Exception as e:
            logging.warning(f"{name} failed to converge during grid search: {e}")
            print(f"{name} failed to converge during grid search: {e}")
            continue
        
        # Training set evaluation
        y_pred_train = model.predict_proba(X_train_selected)[:, 1]
        auc_train = roc_auc_score(y_train, y_pred_train)
        
        # Bootstrap calculation for training AUC CI
        def compute_auc_ci(y_true, y_pred, n_bootstraps=2000, seed=42):
            rng = np.random.RandomState(seed)
            def auc_statistic(y, y_pred):
                return roc_auc_score(y, y_pred)
            boot_result = bootstrap((y_true, y_pred), statistic=auc_statistic, n_resamples=n_bootstraps, random_state=rng, paired=True)
            ci = boot_result.confidence_interval
            return np.array([ci.low, ci.high])
        
        # Ensure y_train is in numpy array format
        y_train_array = y_train.to_numpy() if isinstance(y_train, pd.Series) else y_train
        ci_train = compute_auc_ci(y_train_array, y_pred_train)
        
        # Additional training metrics
        y_pred_class_train = model.predict(X_train_selected)
        report_train = classification_report(y_train, y_pred_class_train, output_dict=True)
        
        # Calculate ROC curve for training
        fpr_train, tpr_train, _ = roc_curve(y_train, y_pred_train)
        fpr_smooth_train = np.linspace(0, 1, 200)
        tpr_smooth_train = np.interp(fpr_smooth_train, fpr_train, tpr_train)
        roc_curves_train[name] = (fpr_smooth_train, tpr_smooth_train, auc_train, ci_train)

        # Test set evaluation
        y_pred_test = model.predict_proba(X_test_selected)[:, 1]
        auc_test = roc_auc_score(y_test, y_pred_test)
        y_pred_tests[name] = y_pred_test  # Store for DeLong test
        
        # Ensure y_test is in numpy array format
        y_test_array = y_test.to_numpy() if isinstance(y_test, pd.Series) else y_test
        ci_test = compute_auc_ci(y_test_array, y_pred_test)
        
        # Additional test metrics
        y_pred_class_test = model.predict(X_test_selected)
        report_test = classification_report(y_test, y_pred_class_test, output_dict=True)
        
        # Calculate ROC curve for testing
        fpr_test, tpr_test, _ = roc_curve(y_test, y_pred_test)
        fpr_smooth_test = np.linspace(0, 1, 200)
        tpr_smooth_test = np.interp(fpr_smooth_test, fpr_test, tpr_test)
        roc_curves_test[name] = (fpr_smooth_test, tpr_smooth_test, auc_test, ci_test)

        # Store results
        results[name] = {
            'Best Parameters': str(grid_search.best_params_),
            'Train AUC': auc_train,
            'Train AUC CI (95%)': f'[{ci_train[0]:.3f}-{ci_train[1]:.3f}]',
            'Train Precision (Class 0)': report_train['0']['precision'],
            'Train Recall (Class 0)': report_train['0']['recall'],
            'Train F1-Score (Class 0)': report_train['0']['f1-score'],
            'Train Precision (Class 1)': report_train['1']['precision'],
            'Train Recall (Class 1)': report_train['1']['recall'],
            'Train F1-Score (Class 1)': report_train['1']['f1-score'],
            'Test AUC': auc_test,
            'Test AUC CI (95%)': f'[{ci_test[0]:.3f}-{ci_test[1]:.3f}]',
            'Test Precision (Class 0)': report_test['0']['precision'],
            'Test Recall (Class 0)': report_test['0']['recall'],
            'Test F1-Score (Class 0)': report_test['0']['f1-score'],
            'Test Precision (Class 1)': report_test['1']['precision'],
            'Test Recall (Class 1)': report_test['1']['recall'],
            'Test F1-Score (Class 1)': report_test['1']['f1-score']
        }
        logging.info(f"{name} - Train AUC: {auc_train:.3f}, CI: {ci_train[0]:.3f}-{ci_train[1]:.3f}, Test AUC: {auc_test:.3f}, CI: {ci_test[0]:.3f}-{ci_test[1]:.3f}")
        print(f"{name} - Train AUC: {auc_train:.3f} (CI: {ci_train[0]:.3f}-{ci_train[1]:.3f}), Test AUC: {auc_test:.3f} (CI: {ci_test[0]:.3f}-{ci_test[1]:.3f})")

    # Implement DeLong test
    def delong_roc_variance(ground_truth, predictions_one, predictions_two):
        """
        Calculate variance for DeLong test
        
        Parameters:
        ground_truth: True binary labels
        predictions_one: Predicted scores from first model
        predictions_two: Predicted scores from second model
        
        Returns:
        variance: Variance for DeLong test
        """
        # Convert inputs to numpy arrays
        ground_truth = np.array(ground_truth)
        predictions_one = np.array(predictions_one)
        predictions_two = np.array(predictions_two)
        
        # Get positive and negative indices
        pos_indices = np.where(ground_truth == 1)[0]
        neg_indices = np.where(ground_truth == 0)[0]
        
        # Get number of positive and negative samples
        n_pos = len(pos_indices)
        n_neg = len(neg_indices)
        
        # Extract predictions for positive and negative classes
        pos_one = predictions_one[pos_indices]
        neg_one = predictions_one[neg_indices]
        pos_two = predictions_two[pos_indices]
        neg_two = predictions_two[neg_indices]
        
        # Initialize covariance terms
        v01 = 0
        v10 = 0
        
        # Calculate covariance terms
        for i in range(n_pos):
            for j in range(n_neg):
                v01 += (int(pos_one[i] > neg_one[j]) - int(pos_two[i] > neg_two[j])) ** 2
                v10 += (int(pos_one[i] <= neg_one[j]) - int(pos_two[i] <= neg_two[j])) ** 2
        
        # Calculate variance
        variance = (v01 + v10) / (n_pos * n_neg)
        
        return variance

    def delong_test(ground_truth, predictions_one, predictions_two):
        """
        Implement DeLong test to compare two ROC curves' AUCs
        
        Parameters:
        ground_truth: True binary labels
        predictions_one: Predicted scores from first model
        predictions_two: Predicted scores from second model
        
        Returns:
        p_value: Two-tailed p-value indicating statistical significance
        z_score: Z-score for the test
        auc_diff: Difference in AUC between the two models
        """
        # Calculate AUCs for both models
        auc_one = roc_auc_score(ground_truth, predictions_one)
        auc_two = roc_auc_score(ground_truth, predictions_two)
        
        # Get variance
        variance = delong_roc_variance(ground_truth, predictions_one, predictions_two)
        
        # Calculate z-score
        z_score = (auc_one - auc_two) / np.sqrt(variance)
        
        # Calculate p-value (two-tailed test)
        p_value = 2 * (1 - stats.norm.cdf(abs(z_score)))
        
        return p_value, z_score, auc_one - auc_two

    # Perform DeLong test
    logging.info("Performing DeLong test to compare model AUC differences")
    print("Performing DeLong test to compare model AUC differences:")
    delong_results = {}
    model_names = list(y_pred_tests.keys())
    for i in range(len(model_names)):
        for j in range(i + 1, len(model_names)):
            model1, model2 = model_names[i], model_names[j]
            try:
                p_value, z_score, auc_diff = delong_test(y_test_array, y_pred_tests[model1], y_pred_tests[model2])
                delong_results[f"{model1} vs {model2}"] = {
                    'p_value': p_value, 
                    'z_score': z_score,
                    'auc_diff': auc_diff
                }
                significance = "Significant" if p_value < 0.05 else "Not significant"
                print(f"DeLong test: {model1} vs {model2}, p-value = {p_value:.4f}, z-score = {z_score:.4f}, AUC difference = {auc_diff:.4f} ({significance})")
                logging.info(f"DeLong test: {model1} vs {model2}, p-value = {p_value:.4f}, z-score = {z_score:.4f}, AUC difference = {auc_diff:.4f} ({significance})")
            except Exception as e:
                print(f"DeLong test failed: {model1} vs {model2}, error: {e}")
                logging.error(f"DeLong test failed: {model1} vs {model2}, error: {e}")

    # Create DeLong test results table
    delong_df = pd.DataFrame([
        {
            'Model Comparison': key,
            'AUC Difference': value['auc_diff'],
            'Z-Score': value['z_score'],
            'P-Value': value['p_value'],
            'Significance': 'Yes (p<0.05)' if value['p_value'] < 0.05 else 'No'
        }
        for key, value in delong_results.items()
    ])
    delong_df = delong_df.sort_values('P-Value')
    print("\nTable 5: DeLong Test Results for AUC Comparison Between Models")
    print(delong_df.to_markdown(tablefmt='grid', floatfmt='.4f'))
    delong_df.to_csv('delong_test_results.csv', index=False)
    logging.info("DeLong test results saved to 'delong_test_results.csv'")

    # Sort ROC curves by AUC in descending order
    roc_curves_train_sorted = dict(sorted(roc_curves_train.items(), key=lambda x: x[1][2], reverse=True))
    roc_curves_test_sorted = dict(sorted(roc_curves_test.items(), key=lambda x: x[1][2], reverse=True))

    # Plot training set ROC curves
    plt.figure(figsize=(12, 8), facecolor='#f5f5f5')
    plt.gca().set_facecolor('#f5f5f5')
    for name, (fpr, tpr, auc, ci) in roc_curves_train_sorted.items():
        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f}, 95% CI: {ci[0]:.3f}-{ci[1]:.3f})', 
                color=model_colors[name], linewidth=3, alpha=0.8,
                path_effects=[path_effects.Stroke(linewidth=4, foreground='black', alpha=0.2), path_effects.Normal()])
    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess', linewidth=1.5, alpha=0.7)
    plt.xlabel('False Positive Rate', fontsize=14)
    plt.ylabel('True Positive Rate', fontsize=14)
    plt.title('ROC Curve Comparison (Training Set, n=400)', fontsize=16, pad=15)
    plt.legend(title='Models (Sorted by AUC)', loc='lower right', fontsize=12, frameon=True, bbox_to_anchor=(0.98, 0.02),
            edgecolor='black', shadow=True)
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig('roc_curve_comparison_train.png', dpi=300, bbox_inches='tight')
    logging.info("ROC curve comparison plot for training set saved as 'roc_curve_comparison_train.png'.")
    plt.show()

    # Plot test set ROC curves
    plt.figure(figsize=(12, 8), facecolor='#f5f5f5')
    plt.gca().set_facecolor('#f5f5f5')
    for name, (fpr, tpr, auc, ci) in roc_curves_test_sorted.items():
        plt.plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f}, 95% CI: {ci[0]:.3f}-{ci[1]:.3f})', 
                color=model_colors[name], linewidth=3, alpha=0.8,
                path_effects=[path_effects.Stroke(linewidth=4, foreground='black', alpha=0.2), path_effects.Normal()])
    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess', linewidth=1.5, alpha=0.7)
    plt.xlabel('False Positive Rate', fontsize=14)
    plt.ylabel('True Positive Rate', fontsize=14)
    plt.title('ROC Curve Comparison (Test Set, n=100)', fontsize=16, pad=15)
    plt.legend(title='Models (Sorted by AUC)', loc='lower right', fontsize=12, frameon=True, bbox_to_anchor=(0.98, 0.02),
            edgecolor='black', shadow=True)
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    plt.savefig('roc_curve_comparison_test.png', dpi=300, bbox_inches='tight')
    logging.info("ROC curve comparison plot for test set saved as 'roc_curve_comparison_test.png'.")
    plt.show()

    # Results table
    results_df = pd.DataFrame(results).T
    results_df.index.name = 'Model'
    # Sort by test set AUC in descending order
    results_df = results_df.sort_values(by='Test AUC', ascending=False)
    print("Table 4: Model Performance Comparison (Training and Test Sets)")
    print(results_df.round(3).to_markdown(tablefmt='grid', floatfmt='.3f'))
    results_df.round(3).to_csv('model_performance_comparison_full.csv', index=True)
    logging.info("Model performance comparison table saved as 'model_performance_comparison_full.csv'.")
    print("Note: Training set includes 202 extravasation and 198 non-extravasation cases; Test set includes 50 extravasation and 50 non-extravasation cases. Values are rounded to 3 decimal places. Class 0 refers to non-extravasation, Class 1 refers to the extravasation group.")
    
    # Determine best model based on test AUC
    best_model_name = results_df.index[0]
    logging.info(f"Best performing model: {best_model_name}")
    
    return best_model_name, models, y_pred_tests


# ===============================
# Part 6: Model Performance Validation
# ===============================

from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import roc_auc_score, roc_curve, brier_score_loss
from sklearn.calibration import calibration_curve
from scipy.stats import chi2

def validate_best_model(best_model_name, models, X_train_selected, X_test_selected, y_train, y_test, save_dir=None):
    """
    Perform comprehensive performance validation for the best model
    
    Parameters:
    best_model_name: Name of the best performing model
    models: Dictionary of trained models
    X_train_selected, X_test_selected: Training and test data with selected features
    y_train, y_test: Training and test targets
    save_dir: Directory for saving results
    
    Returns:
    model: Validated best model
    """
    # Set up logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logging.info(f"Initiating comprehensive performance evaluation for the best model ({best_model_name}).")

    # Print current working directory to confirm running environment
    current_dir = os.getcwd()
    logging.info(f"Current working directory: {current_dir}")

    # Specify save directory
    save_dir = save_dir or "/Users/username/results"
    # Ensure save directory exists
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
        logging.info(f"Created directory: {save_dir}")
    else:
        logging.info(f"Save directory already exists: {save_dir}")

    # Verify consistency of test set
    if len(y_test) != X_test_selected.shape[0]:
        logging.warning(f"Length mismatch detected: y_test has {len(y_test)} samples, X_test_selected has {X_test_selected.shape[0]} samples.")
        min_length = min(len(y_test), X_test_selected.shape[0])
        y_test = y_test[:min_length]
        X_test_selected = X_test_selected[:min_length]
        logging.info(f"Aligned test set to {min_length} samples by truncating the larger array.")
    else:
        logging.info(f"Test set lengths are consistent: {len(y_test)} samples.")

    # Verify input data
    if not np.all(np.isin(y_test, [0, 1])):
        logging.warning("y_test contains non-binary values. Converting to binary if possible.")
        y_test = (y_test > 0.5).astype(int)
    if np.any(np.isnan(X_test_selected)):
        logging.warning("NaN values detected in X_test_selected. Replacing with 0.")
        X_test_selected = np.nan_to_num(X_test_selected, nan=0.0)

    # Define intuitive feature name mappings
    intuitive_names = {
        'vein_quality_1': 'Vein Quality (Fair)',
        'vein_quality_2': 'Vein Quality (Poor)',
        'vein_quality_3': 'Vein Quality (Very Poor)',
        'needle_site_2': 'Needle Site (Forearm/Upper Arm)',
        'needle_site_3': 'Needle Site (Wrist)',
        'needle_site_4': 'Needle Site (Hand)',
        'needle_gauge_2': 'Needle Gauge (20G)',
        'needle_gauge_3': 'Needle Gauge (18G)',
        'active_ingredient_2': 'Active Ingredient (Iomeprol)',
        'active_ingredient_3': 'Active Ingredient (Iopromide)',
        'active_ingredient_4': 'Active Ingredient (Iopamidol)',
        'chemotherapy_history': 'Chemotherapy History',
        'needle_type': 'Needle Type',
        'needle_insertion_depth': 'Needle Insertion Depth',
        'other_puncture': 'Other Puncture',
        'gender': 'Gender',
        'malignancy': 'Malignancy',
        'diabetes': 'Diabetes',
        'hypertension': 'Hypertension',
        'needle_dwell_time': 'Needle Dwell Time',
        'needle_prior_use': 'Needle Prior Use',
        'pre_procedure_pain': 'Pre-procedure Pain',
        'needle_placement': 'Needle Placement',
        'patient_origin': 'Patient Origin',
        'temp_diff': 'Temperature Difference',
        'age': 'Age',
        'bmi': 'BMI',
        'contrast_dose': 'Contrast Dose',
        'injection_rate': 'Injection Rate'
    }

    # Define binary feature value mappings (for force plots)
    value_mappings = {
        'Needle Type': {0: 'Straight', 1: 'Y-type'},
        'Gender': {0: 'Male', 1: 'Female'},
        'Pre-procedure Pain': {0: 'No', 1: 'Yes'},
        'Needle Insertion Depth': {0: 'Not Fully Inserted', 1: 'Fully Inserted'},
        'Needle Site (Wrist)': {0: 'Not Wrist', 1: 'Wrist'},
        'Needle Site (Forearm/Upper Arm)': {0: 'Not Forearm/Upper Arm', 1: 'Forearm/Upper Arm'},
        'Needle Site (Hand)': {0: 'Not Hand', 1: 'Hand'},
        'Chemotherapy History': {0: 'No', 1: 'Yes'},
        'Other Puncture': {0: 'No', 1: 'Yes'},
        'Malignancy': {0: 'No', 1: 'Yes'},
        'Diabetes': {0: 'No', 1: 'Yes'},
        'Hypertension': {0: 'No', 1: 'Yes'},
        'Needle Prior Use': {0: 'No', 1: 'Yes'},
        'Needle Placement': {0: 'Left', 1: 'Right'},
        'Patient Origin': {0: 'Inpatient', 1: 'Outpatient'},
        'Vein Quality (Fair)': {0: 'No', 1: 'Yes'},
        'Vein Quality (Poor)': {0: 'No', 1: 'Yes'},
        'Vein Quality (Very Poor)': {0: 'No', 1: 'Yes'},
        'Needle Gauge (20G)': {0: 'No', 1: 'Yes'},
        'Needle Gauge (18G)': {0: 'No', 1: 'Yes'},
        'Active Ingredient (Iomeprol)': {0: 'No', 1: 'Yes'},
        'Active Ingredient (Iopromide)': {0: 'No', 1: 'Yes'},
        'Active Ingredient (Iopamidol)': {0: 'No', 1: 'Yes'}
    }

    # Map selected_features to intuitive names
    intuitive_feature_names = [intuitive_names.get(name, name) for name in selected_features]
    logging.info(f"Intuitive feature names: {intuitive_feature_names}")

    # Create two DataFrames: one for SHAP distribution plots (preserving numeric values), one for SHAP force plots (displaying intuitive names)
    X_test_df = pd.DataFrame(X_test_selected, columns=intuitive_feature_names)
    X_test_df_numeric = pd.DataFrame(X_test_selected, columns=intuitive_feature_names)

    # Apply value mappings to X_test_df (for SHAP force plots)
    for idx, feature in enumerate(intuitive_feature_names):
        if feature in value_mappings:
            logging.info(f"Mapping values for feature: {feature}")
            X_test_df[feature] = X_test_df[feature].map(value_mappings[feature])
            # Check mapped values
            mapped_values = X_test_df[feature].unique()
            logging.info(f"After mapping, unique values in '{feature}' column: {mapped_values}")
            if X_test_df[feature].isnull().any():
                logging.warning(f"After mapping, '{feature}' contains NaN values. Please check the data.")

    # Create SHAP explainer (LinearExplainer for logistic regression)
    explainer = shap.LinearExplainer(model, X_train_selected, feature_perturbation="interventional")
    shap_values_path = os.path.join(save_dir, 'shap_values.pkl')

    # Delete old SHAP values file, recalculate
    if os.path.exists(shap_values_path):
        os.remove(shap_values_path)
        logging.info(f"Deleted old SHAP values file: '{shap_values_path}'.")

    logging.info("Computing SHAP values...")
    shap_values = explainer.shap_values(X_test_selected)
    # Save SHAP values (ensure original array is saved)
    with open(shap_values_path, 'wb') as f:
        pickle.dump(shap_values, f)
    logging.info(f"SHAP values saved to '{shap_values_path}'.")

    # Extract high-risk and low-risk patient SHAP values
    y_pred_test_prob = model.predict_proba(X_test_selected)[:, 1]
    high_risk_idx = np.argmax(y_pred_test_prob)  # Sample with highest predicted probability
    low_risk_idx = np.argmin(y_pred_test_prob)  # Sample with lowest predicted probability

    # 1. Global feature importance plot (Summary Plot)
    plt.figure(figsize=(10, 8), facecolor='#f5f5f5')
    plt.gca().set_facecolor('#f5f5f5')
    shap.summary_plot(shap_values, X_test_df, plot_type="bar", show=False, color='blue')
    plt.title('Feature Importance (SHAP Values, Test Set, n=100)', fontsize=16, pad=15)
    plt.xlabel('Mean(|SHAP Value|) (Average Impact on Model Output Magnitude)', fontsize=14)
    plt.ylabel('Feature', fontsize=14)
    plt.yticks(fontsize=12)
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'shap_feature_importance.png'), dpi=300, bbox_inches='tight')
    logging.info(f"SHAP feature importance plot saved as '{os.path.join(save_dir, 'shap_feature_importance.png')}'.")
    plt.show()
    plt.close()

    # 2. Global SHAP scatter plot (showing relationship between feature values and SHAP values)
    plt.figure(figsize=(10, 8), facecolor='#f5f5f5')
    plt.gca().set_facecolor('#f5f5f5')
    shap.summary_plot(shap_values, X_test_df_numeric, show=False)
    plt.title('SHAP Values Distribution (Test Set, n=100)', fontsize=16, pad=15)
    plt.xlabel('SHAP Value (Impact on Model Output)', fontsize=14)
    plt.ylabel('Feature', fontsize=14)
    plt.yticks(fontsize=12)
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'shap_values_distribution.png'), dpi=300, bbox_inches='tight')
    logging.info(f"SHAP values distribution plot saved as '{os.path.join(save_dir, 'shap_values_distribution.png')}'.")
    plt.show()
    plt.close()

    # 3. Individual SHAP explanation (high-risk sample)
    plt.figure(figsize=(12, 4))
    shap.force_plot(explainer.expected_value, shap_values[high_risk_idx], X_test_df.iloc[high_risk_idx], 
                   matplotlib=True, show=False, contribution_threshold=0.1)
    plt.title(f'SHAP Force Plot for High-Risk Patient (Predicted Probability: {y_pred_test_prob[high_risk_idx]:.3f})', fontsize=14, pad=15)
    plt.tight_layout()
    high_risk_save_path = os.path.join(save_dir, 'shap_force_plot_high_risk.png')
    plt.savefig(high_risk_save_path, dpi=300, bbox_inches='tight')
    logging.info(f"SHAP force plot for high-risk patient saved as '{high_risk_save_path}'.")
    plt.show()
    plt.close()

    # 4. Individual SHAP explanation (low-risk sample)
    plt.figure(figsize=(12, 4))
    shap.force_plot(explainer.expected_value, shap_values[low_risk_idx], X_test_df.iloc[low_risk_idx], 
                   matplotlib=True, show=False, contribution_threshold=0.1)
    plt.title(f'SHAP Force Plot for Low-Risk Patient (Predicted Probability: {y_pred_test_prob[low_risk_idx]:.3f})', fontsize=14, pad=15)
    plt.tight_layout()
    low_risk_save_path = os.path.join(save_dir, 'shap_force_plot_low_risk.png')
    plt.savefig(low_risk_save_path, dpi=300, bbox_inches='tight')
    logging.info(f"SHAP force plot for low-risk patient saved as '{low_risk_save_path}'.")
    plt.show()
    plt.close()

    # 5. Feature contribution table (Table 7)
    shap_df = pd.DataFrame(shap_values, columns=intuitive_feature_names)
    shap_mean_abs = np.abs(shap_df).mean().sort_values(ascending=False)
    shap_table = pd.DataFrame({
        'Feature': shap_mean_abs.index,
        'Mean Absolute SHAP Value': shap_mean_abs.values
    })
    print("Table 7: Feature Contributions to Predictions (SHAP Values, Test Set, n=100)")
    print(shap_table.to_markdown(tablefmt='grid', floatfmt='.3f', colalign=("center", "center")))
    shap_table_save_path = os.path.join(save_dir, 'shap_feature_contributions.csv')
    shap_table.to_csv(shap_table_save_path, index=False)
    logging.info(f"SHAP feature contributions table saved as '{shap_table_save_path}'.")

    # Verify saved files
    for file_name in ['shap_feature_importance.png', 'shap_values_distribution.png', 'shap_force_plot_high_risk.png', 'shap_force_plot_low_risk.png']:
        file_path = os.path.join(save_dir, file_name)
        if os.path.exists(file_path):
            file_size = os.path.getsize(file_path)
            logging.info(f"File '{file_path}' exists with size {file_size} bytes.")
            if file_size == 0:
                logging.warning(f"File '{file_path}' is empty.")
        else:
            logging.warning(f"File '{file_path}' does not exist. Save operation may have failed.")

    logging.info("SHAP explanation with intuitive feature names completed successfully.")
    
    return shap_values


# ===============================
# Part 8: Main Function
# ===============================

def main(data_file_path=None, save_dir=None):
    """
    Main function to run the complete workflow
    
    Parameters:
    data_file_path: Path to the input data file
    save_dir: Directory for saving results
    
    Returns:
    model: Trained and validated model
    """
    # Set up logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logging.info("Starting contrast media extravasation prediction model workflow.")
    
    # Set default save directory
    save_dir = save_dir or "/Users/username/results"
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
        logging.info(f"Created save directory: {save_dir}")
    
    # Step 1: Data Preprocessing
    logging.info("Step 1: Data Preprocessing")
    X_train, X_test, y_train, y_test, feature_names = preprocess_data(data_file_path)
    logging.info(f"Preprocessing complete. Training data shape: {X_train.shape}, Test data shape: {X_test.shape}")
    
    # Step 2: Generate Descriptive Statistics
    logging.info("Step 2: Generating Descriptive Statistics")
    table1 = generate_descriptive_statistics(data_file_path)
    logging.info("Descriptive statistics generated and saved.")
    
    # Step 3: Feature Selection
    logging.info("Step 3: Feature Selection")
    X_train_selected, X_test_selected, selected_features = select_features(X_train, y_train, X_test, feature_names)
    logging.info(f"Feature selection complete. Selected {len(selected_features)} features.")
    
    # Step 4: LASSO Visualization
    logging.info("Step 4: LASSO Visualization")
    lasso_model = visualize_lasso(X_train_selected, y_train, selected_features)
    logging.info("LASSO visualization complete.")
    
    # Step 5: Model Training and Evaluation
    logging.info("Step 5: Model Training and Evaluation")
    best_model_name, models, y_pred_tests = train_and_evaluate_models(X_train_selected, X_test_selected, y_train, y_test)
    logging.info(f"Model evaluation complete. Best model: {best_model_name}")
    
    # Step 6: Best Model Validation
    logging.info("Step 6: Best Model Validation")
    best_model = validate_best_model(best_model_name, models, X_train_selected, X_test_selected, y_train, y_test, save_dir)
    logging.info("Best model validation complete.")
    
    # Step 7: Model Explanation
    logging.info("Step 7: Model Explanation with SHAP")
    shap_values = explain_model(best_model, X_train_selected, X_test_selected, y_train, y_test, selected_features, save_dir)
    logging.info("Model explanation complete.")
    
    # Save final model
    model_path = os.path.join(save_dir, 'final_model.pkl')
    with open(model_path, 'wb') as f:
        pickle.dump(best_model, f)
    logging.info(f"Final model saved to {model_path}")
    
    logging.info("Contrast media extravasation prediction model workflow completed successfully.")
    
    return best_model


# Execute main function if this script is run directly
if __name__ == "__main__":
    # Get command line arguments
    import argparse
    parser = argparse.ArgumentParser(description='Run contrast media extravasation prediction model.')
    parser.add_argument('--data_file', type=str, help='Path to input data file')
    parser.add_argument('--save_dir', type=str, help='Directory for saving results')
    args = parser.parse_args()
    
    # Run main function
    model = main(args.data_file, args.save_dir)

    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
        logging.info(f"Created directory: {save_dir}")
    else:
        logging.info(f"Save directory already exists: {save_dir}")

    # Ensure y_train and y_test are numpy arrays
    y_train = y_train.to_numpy() if isinstance(y_train, pd.Series) else y_train
    y_test = y_test.to_numpy() if isinstance(y_test, pd.Series) else y_test

    # Verify test set sample consistency
    if len(y_test) != X_test_selected.shape[0]:
        logging.warning(f"Length mismatch detected: y_test has {len(y_test)} samples, X_test_selected has {X_test_selected.shape[0]} samples.")
        min_length = min(len(y_test), X_test_selected.shape[0])
        y_test = y_test[:min_length]
        X_test_selected = X_test_selected[:min_length]
        logging.info(f"Aligned test set to {min_length} samples by truncating the larger array.")
    else:
        logging.info(f"Test set lengths are consistent: {len(y_test)} samples.")

    # Verify input data
    if not np.all(np.isin(y_train, [0, 1])) or not np.all(np.isin(y_test, [0, 1])):
        logging.warning("Non-binary values detected in y_train or y_test. Converting to binary.")
        y_train = (y_train > 0.5).astype(int)
        y_test = (y_test > 0.5).astype(int)

    # Get best model
    model = models[best_model_name]

    # 1. Internal validation: 5-fold cross-validation (training set)
    kf = KFold(n_splits=5, shuffle=True, random_state=42)
    cv_auc_scores = cross_val_score(model, X_train_selected, y_train, cv=kf, scoring='roc_auc')
    cv_brier_scores = []
    cv_hl_p_values = []
    cv_predictions = np.zeros(len(y_train))
    for train_idx, val_idx in kf.split(X_train_selected):
        X_train_fold, X_val_fold = X_train_selected[train_idx], X_train_selected[val_idx]
        y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]
        model.fit(X_train_fold, y_train_fold)
        y_pred_prob = model.predict_proba(X_val_fold)[:, 1]
        cv_predictions[val_idx] = y_pred_prob
        cv_brier_scores.append(brier_score_loss(y_val_fold, y_pred_prob))
        # Hosmer-Lemeshow test
        y_pred_decile = pd.qcut(y_pred_prob, 10, labels=False, duplicates='drop')
        hl_table = pd.crosstab(y_pred_decile, y_val_fold)
        obs = hl_table[1].values if 1 in hl_table.columns else np.zeros(len(hl_table))
        exp = hl_table.sum(axis=1) * np.mean(y_val_fold)
        hl_stat = np.sum((obs - exp) ** 2 / (exp + 0.001))
        hl_p = 1 - chi2.cdf(hl_stat, len(hl_table) - 1) if not np.isnan(hl_stat) else np.nan
        cv_hl_p_values.append(hl_p)

    cv_auc_mean = np.mean(cv_auc_scores)
    cv_auc_std = np.std(cv_auc_scores)
    cv_auc_ci = [cv_auc_mean - 1.96 * cv_auc_std / np.sqrt(5), cv_auc_mean + 1.96 * cv_auc_std / np.sqrt(5)]
    cv_brier_mean = np.mean(cv_brier_scores)
    cv_hl_mean_p = np.nanmean(cv_hl_p_values)
    logging.info(f"Cross-validation results: AUC = {cv_auc_mean:.3f} (95% CI: {cv_auc_ci[0]:.3f}-{cv_auc_ci[1]:.3f}), "
                f"Brier Score = {cv_brier_mean:.3f}, Mean HL p-value = {cv_hl_mean_p:.3f}")

    # Training set ROC, calibration, and DCA
    fpr_train, tpr_train, thresholds_train = roc_curve(y_train, cv_predictions)
    fpr_smooth_train = np.linspace(0, 1, 200)
    tpr_smooth_train = np.interp(fpr_smooth_train, fpr_train, tpr_train)
    fraction_of_positives_train, mean_predicted_value_train = calibration_curve(y_train, cv_predictions, n_bins=10)
    bin_counts_train = np.histogram(cv_predictions, bins=10)[0]
    mask_train = bin_counts_train >= 10
    bin_std_train = np.sqrt(fraction_of_positives_train * (1 - fraction_of_positives_train) / bin_counts_train)
    thresholds_dca_train = np.arange(0.05, 0.95, 0.01)
    net_benefit_train = []
    net_benefit_treat_all_train = []
    net_benefit_treat_none_train = []
    p_train = np.mean(y_train)
    n_train = len(y_train)
    for t in thresholds_dca_train:
        y_pred_binary = (cv_predictions >= t).astype(int)
        tp = np.sum((y_pred_binary == 1) & (y_train == 1))
        fp = np.sum((y_pred_binary == 1) & (y_train == 0))
        net_benefit_t = (tp / n_train) - (fp / n_train) * (t / (1 - t)) if (1 - t) > 0 else 0
        net_benefit_train.append(net_benefit_t)
        net_benefit_treat_all_train.append(p_train - (1 - p_train) * (t / (1 - t)) if (1 - t) > 0 else 0)
        net_benefit_treat_none_train.append(0)
    optimal_threshold_train = thresholds_dca_train[np.argmax(net_benefit_train)] if np.max(net_benefit_train) > 0 else 0.5

    # 2. Test set performance
    model.fit(X_train_selected, y_train)
    y_pred_test_prob = model.predict_proba(X_test_selected)[:, 1]
    if np.any(np.isnan(y_pred_test_prob)):
        logging.warning("NaN values detected in y_pred_test_prob. Replacing with 0.")
        y_pred_test_prob = np.nan_to_num(y_pred_test_prob, nan=0.0)
    auc_test = roc_auc_score(y_test, y_pred_test_prob)
    np.random.seed(42)
    boot_aucs = []
    n_boot = 5000
    for _ in range(n_boot):
        indices = np.random.randint(0, len(y_test), len(y_test))
        boot_auc = roc_auc_score(y_test[indices], y_pred_test_prob[indices])
        boot_aucs.append(boot_auc)
    boot_ci = np.percentile(boot_aucs, [2.5, 97.5])
    fpr, tpr, thresholds = roc_curve(y_test, y_pred_test_prob)
    fpr_smooth = np.linspace(0, 1, 200)
    tpr_smooth = np.interp(fpr_smooth, fpr, tpr)
    fraction_of_positives_test, mean_predicted_value_test = calibration_curve(y_test, y_pred_test_prob, n_bins=10)
    bin_counts_test = np.histogram(y_pred_test_prob, bins=10)[0]
    mask_test = bin_counts_test >= 10
    bin_std_test = np.sqrt(fraction_of_positives_test * (1 - fraction_of_positives_test) / bin_counts_test)
    brier_test = brier_score_loss(y_test, y_pred_test_prob)
    y_pred_decile_test = pd.qcut(y_pred_test_prob, 10, labels=False, duplicates='drop')
    hl_table_test = pd.crosstab(y_pred_decile_test, y_test)
    obs_test = hl_table_test[1].values if 1 in hl_table_test.columns else np.zeros(len(hl_table_test))
    exp_test = hl_table_test.sum(axis=1) * np.mean(y_test)
    hl_stat_test = np.sum((obs_test - exp_test) ** 2 / (exp_test + 0.001))
    hl_p_test = 1 - chi2.cdf(hl_stat_test, len(hl_table_test) - 1) if not np.isnan(hl_stat_test) else np.nan
    thresholds_dca = np.arange(0.05, 0.95, 0.01)
    net_benefit = []
    net_benefit_treat_all = []
    net_benefit_treat_none = []
    p = np.mean(y_test)
    n = len(y_test)
    for t in thresholds_dca:
        y_pred_binary = (y_pred_test_prob >= t).astype(int)
        tp = np.sum((y_pred_binary == 1) & (y_test == 1))
        fp = np.sum((y_pred_binary == 1) & (y_test == 0))
        net_benefit_t = (tp / n) - (fp / n) * (t / (1 - t)) if (1 - t) > 0 else 0
        net_benefit.append(net_benefit_t)
        net_benefit_treat_all.append(p - (1 - p) * (t / (1 - t)) if (1 - t) > 0 else 0)
        net_benefit_treat_none.append(0)
    optimal_threshold = thresholds_dca[np.argmax(net_benefit)] if np.max(net_benefit) > 0 else 0.5
    logging.info(f"Test set results: AUC = {auc_test:.3f} (95% CI: {boot_ci[0]:.3f}-{boot_ci[1]:.3f}), "
                f"Brier Score = {brier_test:.3f}, HL p-value = {hl_p_test:.3f}, Optimal DCA Threshold = {optimal_threshold:.2f}")

    # 3. Plot combined ROC curves (training and test sets using different colors, dark blue and orange)
    plt.figure(figsize=(10, 6), facecolor='#f5f5f5')
    plt.gca().set_facecolor('#f5f5f5')
    plt.plot(fpr_smooth_train, tpr_smooth_train, label=f'Training Set (AUC = {cv_auc_mean:.2f}, 95% CI: {cv_auc_ci[0]:.3f}-{cv_auc_ci[1]:.3f})', 
            color='#1f77b4', linestyle='--', linewidth=3,
            path_effects=[path_effects.Stroke(linewidth=4, foreground='black', alpha=0.2), path_effects.Normal()])
    plt.plot(fpr_smooth, tpr_smooth, label=f'Test Set (AUC = {auc_test:.2f}, 95% CI: {boot_ci[0]:.3f}-{boot_ci[1]:.3f})', 
            color='#ff7f0e', linestyle='-', linewidth=3,
            path_effects=[path_effects.Stroke(linewidth=4, foreground='black', alpha=0.2), path_effects.Normal()])
    # Training set threshold markers
    train_indices = [0, len(thresholds_train) // 4, len(thresholds_train) // 2, len(thresholds_train) - 1]
    for i in train_indices:
        if i < len(thresholds_train):  # Boundary check
            thresh = thresholds_train[i]
            if 0 < thresh < 1:
                plt.plot(fpr_train[i], tpr_train[i], 'o', color='#1f77b4', markersize=6)
                ha = 'left' if fpr_train[i] < 0.5 else 'right'
                plt.text(fpr_train[i], tpr_train[i], f'{thresh:.2f}', fontsize=8, ha=ha, va='bottom', color='#1f77b4')
    # Test set threshold markers
    test_indices = [0, len(thresholds) // 4, len(thresholds) // 2, len(thresholds) - 1]
    for i in test_indices:
        if i < len(thresholds):  # Boundary check
            thresh = thresholds[i]
            if 0 < thresh < 1:
                plt.plot(fpr[i], tpr[i], 'o', color='#ff7f0e', markersize=6)
                ha = 'left' if fpr[i] < 0.5 else 'right'
                plt.text(fpr[i], tpr[i], f'{thresh:.2f}', fontsize=8, ha=ha, va='bottom', color='#ff7f0e')
    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess', linewidth=1.5, alpha=0.7)
    plt.xlabel('False Positive Rate', fontsize=14)
    plt.ylabel('True Positive Rate', fontsize=14)
    plt.title(f'ROC Curves ({best_model_name})', fontsize=16, pad=15)
    plt.legend(loc='lower right', fontsize=12, frameon=True, bbox_to_anchor=(0.98, 0.02),
            edgecolor='black', shadow=True)
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    try:
        roc_save_path = os.path.join(save_dir, 'roc_curve_combined.png')
        print("About to save ROC curve...")
        plt.savefig(roc_save_path, dpi=300, bbox_inches='tight')
        print("ROC curve saved.")
        logging.info(f"Combined ROC curve saved as '{roc_save_path}'.")
        if os.path.exists(roc_save_path):
            file_size = os.path.getsize(roc_save_path)
            logging.info(f"Verified: File '{roc_save_path}' exists with size {file_size} bytes.")
            if file_size == 0:
                logging.warning(f"File '{roc_save_path}' is empty.")
        else:
            logging.warning(f"File '{roc_save_path}' does not exist after saving.")
    except Exception as e:
        logging.error(f"Failed to save ROC curve: {e}")
    plt.show()
    plt.close()

    # 4. Plot combined calibration curves (training and test sets using different colors, dark blue and orange)
    plt.figure(figsize=(10, 6), facecolor='#f5f5f5')
    plt.gca().set_facecolor('#f5f5f5')
    # Training set
    plt.errorbar(mean_predicted_value_train[mask_train], fraction_of_positives_train[mask_train], 
                yerr=bin_std_train[mask_train], fmt='o-', markersize=6, capsize=3, 
                label=f'Training Set (Brier = {cv_brier_mean:.3f})', color='#1f77b4', linestyle='--')
    # Test set
    plt.errorbar(mean_predicted_value_test[mask_test], fraction_of_positives_test[mask_test], 
                yerr=bin_std_test[mask_test], fmt='o-', markersize=6, capsize=3, 
                label=f'Test Set (Brier = {brier_test:.3f})', color='#ff7f0e', linestyle='-')
    plt.plot([0, 1], [0, 1], 'k--', label='Perfect Calibration', linewidth=1.5, alpha=0.7)
    plt.xlabel('Predicted Probability', fontsize=14)
    plt.ylabel('True Probability', fontsize=14)
    plt.title(f'Calibration Curves ({best_model_name})', fontsize=16, pad=15)
    plt.legend(loc='upper left', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    try:
        calibration_save_path = os.path.join(save_dir, 'calibration_curve_combined.png')
        print("About to save calibration curve...")
        plt.savefig(calibration_save_path, dpi=300, bbox_inches='tight')
        print("Calibration curve saved.")
        logging.info(f"Combined calibration curve saved as '{calibration_save_path}'.")
        if os.path.exists(calibration_save_path):
            file_size = os.path.getsize(calibration_save_path)
            logging.info(f"Verified: File '{calibration_save_path}' exists with size {file_size} bytes.")
            if file_size == 0:
                logging.warning(f"File '{calibration_save_path}' is empty.")
        else:
            logging.warning(f"File '{calibration_save_path}' does not exist after saving.")
    except Exception as e:
        logging.error(f"Failed to save calibration curve: {e}")
    plt.show()
    plt.close()

    # 5. Plot combined DCA (training and test sets using different colors, dark blue and orange)
    plt.figure(figsize=(10, 6), facecolor='#f5f5f5')
    plt.gca().set_facecolor('#f5f5f5')
    plt.plot(thresholds_dca_train, net_benefit_train, label=f'{best_model_name} (Training)', color='#1f77b4', linestyle='--', linewidth=2)
    plt.plot(thresholds_dca, net_benefit, label=f'{best_model_name} (Test)', color='#ff7f0e', linestyle='-', linewidth=2)
    plt.plot(thresholds_dca, net_benefit_treat_none, linestyle='--', label='Treat None', color='black')
    plt.plot(thresholds_dca, net_benefit_treat_all, linestyle='--', label='Treat All', color='gray')
    plt.axvline(optimal_threshold_train, color='#1f77b4', linestyle=':', label=f'Optimal Threshold (Training) = {optimal_threshold_train:.2f}')
    plt.axvline(optimal_threshold, color='#ff7f0e', linestyle='-.', label=f'Optimal Threshold (Test) = {optimal_threshold:.2f}')
    plt.text(0.9, max(max(net_benefit_train), max(net_benefit)) * 0.9, 
            f'Net Benefit Range: {max(max(net_benefit_train), max(net_benefit)):.2f} to {min(min(net_benefit_train), min(net_benefit)):.2f}', 
            ha='right', va='top', fontsize=10)
    plt.ylim(-0.5, 0.6)
    plt.xlabel('Threshold Probability', fontsize=14)
    plt.ylabel('Net Benefit', fontsize=14)
    plt.title(f'Decision Curve Analysis ({best_model_name})', fontsize=16, pad=15)
    plt.legend(loc='lower right', fontsize=12, frameon=True, bbox_to_anchor=(0.98, 0.02),
            edgecolor='black', shadow=True)
    plt.grid(True, linestyle='--', alpha=0.5)
    plt.tight_layout()
    try:
        dca_save_path = os.path.join(save_dir, 'decision_curve_combined.png')
        print("About to save decision curve...")
        plt.savefig(dca_save_path, dpi=300, bbox_inches='tight')
        print("Decision curve saved.")
        logging.info(f"Combined decision curve saved as '{dca_save_path}'.")
        if os.path.exists(dca_save_path):
            file_size = os.path.getsize(dca_save_path)
            logging.info(f"Verified: File '{dca_save_path}' exists with size {file_size} bytes.")
            if file_size == 0:
                logging.warning(f"File '{dca_save_path}' is empty.")
        else:
            logging.warning(f"File '{dca_save_path}' does not exist after saving.")
    except Exception as e:
        logging.error(f"Failed to save decision curve: {e}")
    plt.show()
    plt.close()

    # 6. Results tables
    cv_results_df = pd.DataFrame({
        'CV AUC': [f'{cv_auc_mean:.2f} [{cv_auc_ci[0]:.2f}-{cv_auc_ci[1]:.2f}]'],
        'CV Brier Score': [f'{cv_brier_mean:.3f}'],
        'CV HL p-value': [f'{cv_hl_mean_p:.3f}'],
        'Optimal DCA Threshold': [f'{optimal_threshold_train:.2f}']
    }, index=[best_model_name])
    test_results_df = pd.DataFrame({
        'Test AUC': [f'{auc_test:.2f} [{boot_ci[0]:.2f}-{boot_ci[1]:.2f}]'],
        'Test Brier Score': [f'{brier_test:.3f}'],
        'Test HL p-value': [f'{hl_p_test:.3f}'],
        'Optimal DCA Threshold': [f'{optimal_threshold:.2f}']
    }, index=[best_model_name])
    print("Table 5: Cross-Validation Performance (Training Set, n=400)")
    print(cv_results_df.to_markdown(tablefmt='grid', floatfmt='.2f', colalign=("center", "center", "center", "center")))
    try:
        cv_results_df.to_csv(os.path.join(save_dir, 'cv_performance.csv'), index=True)
        logging.info(f"Cross-validation performance table saved as '{os.path.join(save_dir, 'cv_performance.csv')}'.")
    except Exception as e:
        logging.error(f"Failed to save cross-validation performance table: {e}")
    print("Note: AUC reported as mean [95% CI]. HL p-value > 0.05 indicates good calibration.")
    print("\nTable 6: Test Set Performance (n=100)")
    print(test_results_df.to_markdown(tablefmt='grid', floatfmt='.2f', colalign=("center", "center", "center", "center")))
    try:
        test_results_df.to_csv(os.path.join(save_dir, 'test_performance.csv'), index=True)
        logging.info(f"Test set performance table saved as '{os.path.join(save_dir, 'test_performance.csv')}'.")
    except Exception as e:
        logging.error(f"Failed to save test set performance table: {e}")
    print("Note: AUC reported with Bootstrap 95% CI. HL p-value > 0.05 indicates good calibration. Optimal DCA threshold maximizes net benefit.")

    # 7. Verify saved files
    for file_name in ['roc_curve_combined.png', 'calibration_curve_combined.png', 'decision_curve_combined.png']:
        file_path = os.path.join(save_dir, file_name)
        if os.path.exists(file_path):
            file_size = os.path.getsize(file_path)
            logging.info(f"File '{file_path}' exists with size {file_size} bytes.")
            if file_size == 0:
                logging.warning(f"File '{file_path}' is empty.")
        else:
            logging.warning(f"File '{file_path}' does not exist. Save operation may have failed.")

    logging.info("Model performance evaluation completed successfully.")
    
    return model


# ===============================
# Part 7: Model Explanation (SHAP)
# ===============================

import shap
import pickle

def explain_model(model, X_train_selected, X_test_selected, y_train, y_test, selected_features, save_dir=None):
    """
    Generate SHAP explanations for the best model
    
    Parameters:
    model: Trained model to explain
    X_train_selected, X_test_selected: Training and test data with selected features
    y_train, y_test: Training and test targets
    selected_features: List of selected feature names
    save_dir: Directory for saving results
    
    Returns:
    shap_values: SHAP values for test set
    """
    # Set random seed for reproducibility
    np.random.seed(42)

    # Set up logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    logging.info("Initiating SHAP explanation for the best model with intuitive feature names.")

    # Specify save directory
    save_dir = save_dir or "/Users/username/results"
    # Ensure save directory exists
